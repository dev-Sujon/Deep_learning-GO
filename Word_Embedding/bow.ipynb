{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODR0bFgdIWl0kWH+R0OcAo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Bag of Words\n","\n","The Bag of Words model is a way to convert text into numerical features. The basic idea is to represent a text document as a collection of its words, disregarding grammar and word order but keeping multiplicity. Here are the steps to build and use a BoW model:\n","\n","1. **Tokenization**: Split the text into words (tokens).\n","2. **Vocabulary Creation**: Create a list of unique words (vocabulary) from all documents.\n","3. **Vectorization**: Create vectors for each document where each element corresponds to a word in the vocabulary. The value is typically the frequency of the word in the document.\n"],"metadata":{"id":"iEY055INqYqv"}},{"cell_type":"code","source":["# Import necessary libraries\n","import nltk\n","nltk.download('stopwords')\n","import tensorflow as tf\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","import string\n","from nltk.corpus import stopwords\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from collections import Counter\n","from sklearn.feature_extraction.text import CountVectorizer\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ivjWrsYAAe23","executionInfo":{"status":"ok","timestamp":1715957180956,"user_tz":-360,"elapsed":434,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"910a7759-8ac9-4e38-87c2-de463cdbafef"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["### Step1: Preprocessing\n","\n","Before tokenization,it's often useful to preprocess the text to improve the\n","quality of the data.\n","   1. Lowercasing: Convert all characters to lowercase to ensure uniformity.\n","   2. Removing Punctuation: Punctuation marks are generally not useful for BoW.\n","   3. Removing Stop Words: Common words like \"and\", \"the\", \"is\" which do not contribute to the meaning of the document.\n","   4. Stemming/Lemmatization: Reduce words to their root form.\n"],"metadata":{"id":"aH-g8WX8qsDs"}},{"cell_type":"code","source":["stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","# Make sure to download the necessary NLTK data files\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"id":"cb8B0VzKqZYA","executionInfo":{"status":"ok","timestamp":1715957181410,"user_tz":-360,"elapsed":19,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6766f59-8175-486a-961b-5fdf0372bd5e"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["len(stop_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNmukrZLrskR","executionInfo":{"status":"ok","timestamp":1715957181410,"user_tz":-360,"elapsed":17,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"e88927a0-f74f-4b26-d983-4b193559cc6a"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["179"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["\n","\n","# Initialize the stop words and the stemmer\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","\n","def preprocess(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Remove punctuation\n","    text = ''.join([char for char in text if char not in string.punctuation])\n","\n","    # # Tokenize text\n","    tokens = nltk.word_tokenize(text)\n","\n","    # Remove stop words\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # # Stem the tokens\n","    tokens = [stemmer.stem(word) for word in tokens]\n","    return tokens\n","\n","text = \"Hello world! This is a Bag of Words example./////.........#########\"\n","print(preprocess(text))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2m4c48i0tYfI","executionInfo":{"status":"ok","timestamp":1715957181410,"user_tz":-360,"elapsed":14,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"03eb0cd9-8666-4f48-9bc0-e47ff8552741"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['hello', 'world', 'bag', 'word', 'exampl']\n"]}]},{"cell_type":"markdown","source":["### Step 2: Vocabulary Creation\n"],"metadata":{"id":"j1zsEvFI17Fy"}},{"cell_type":"code","source":["corpus = [\n","    \"Hello world! This is a Bag of Words example.\",\n","    \"Bag of Words is a simple model.\",\n","    \"We are learning Bag of Words.\"\n","]\n","\n","processed_corpus = [preprocess(text) for text in corpus]\n","vocabulary = set()\n","for tokens in processed_corpus:\n","  vocabulary.update(tokens)\n","print(vocabulary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4aXH3f8BztMR","executionInfo":{"status":"ok","timestamp":1715957181410,"user_tz":-360,"elapsed":9,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"d7c7c072-b264-4d0b-8468-f3bec43576ff"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["{'hello', 'model', 'simpl', 'world', 'exampl', 'word', 'learn', 'bag'}\n"]}]},{"cell_type":"markdown","source":["### vectorization"],"metadata":{"id":"VZo1KVm88VEt"}},{"cell_type":"code","source":["\n","def vectorize(tokens, vocabulary):\n","    token_count = Counter(tokens)\n","    vector = [token_count[word] for word in vocabulary]\n","    return vector\n","\n","vectors = [vectorize(doc, vocabulary) for doc in processed_corpus]\n","vectors\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ibiTs2xl9ILP","executionInfo":{"status":"ok","timestamp":1715957181410,"user_tz":-360,"elapsed":5,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"17ff0ad5-11fe-4d7e-82dd-a8d9f3667333"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 0, 0, 1, 1, 1, 0, 1], [0, 1, 1, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 1, 1, 1]]"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### Scikit-Learn for Bag of Words"],"metadata":{"id":"ccr930Nw9YL-"}},{"cell_type":"code","source":["vectorizer = CountVectorizer()\n","vectors = vectorizer.fit_transform(corpus)\n","print(vectorizer.vocabulary_)\n","print(vectorizer.get_feature_names_out())\n","print(vectors.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONbTaqiE9Sih","executionInfo":{"status":"ok","timestamp":1715957182215,"user_tz":-360,"elapsed":9,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"662db725-58ad-4b2d-dacb-3ae9bc2ecf0e"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["{'hello': 3, 'world': 12, 'this': 9, 'is': 4, 'bag': 1, 'of': 7, 'words': 11, 'example': 2, 'simple': 8, 'model': 6, 'we': 10, 'are': 0, 'learning': 5}\n","['are' 'bag' 'example' 'hello' 'is' 'learning' 'model' 'of' 'simple'\n"," 'this' 'we' 'words' 'world']\n","[[0 1 1 1 1 0 0 1 0 1 0 1 1]\n"," [0 1 0 0 1 0 1 1 1 0 0 1 0]\n"," [1 1 0 0 0 1 0 1 0 0 1 1 0]]\n"]}]},{"cell_type":"markdown","source":["### One code snippet"],"metadata":{"id":"V15y5Nf_BFFh"}},{"cell_type":"code","source":["# prompt: mount drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZukX5sHYBPLC","executionInfo":{"status":"ok","timestamp":1715957232061,"user_tz":-360,"elapsed":19772,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"5f7bf40d-9faf-4e27-eefd-d54ff88110ad"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/Deep_learning_model/datasets/quora.csv')"],"metadata":{"id":"Nl7QwopHBgvY","executionInfo":{"status":"ok","timestamp":1715957313792,"user_tz":-360,"elapsed":5409,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["questions = df.question1 + df.question2"],"metadata":{"id":"Qm5qleuuBqYJ","executionInfo":{"status":"ok","timestamp":1715957367302,"user_tz":-360,"elapsed":1176,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# prompt: questions.typeof()\n","\n","questions.dtypes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LhAJxhewB0Ce","executionInfo":{"status":"ok","timestamp":1715957449664,"user_tz":-360,"elapsed":471,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"a8a809dd-df20-45b0-c4b8-b7321b2f5467"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dtype('O')"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# prompt: convert questions to str\n","\n","questions = questions[:5000].astype(str)\n"],"metadata":{"id":"2A972EUQCEQC","executionInfo":{"status":"ok","timestamp":1715957632504,"user_tz":-360,"elapsed":424,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# Preprocess the questions\n","processed_questions = [preprocess(question) for question in questions]\n","\n","# Create a vocabulary\n","vocabulary = set()\n","for tokens in processed_questions:\n","  vocabulary.update(tokens)\n","\n","# Vectorize the questions\n","vectors = [vectorize(doc, vocabulary) for doc in processed_questions]\n","\n","# Convert the vectors to a dataframe\n","df_vectors = pd.DataFrame(vectors)\n","\n","# Print the dataframe\n","print(df_vectors.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUYnjC9-CSzP","executionInfo":{"status":"ok","timestamp":1715957636110,"user_tz":-360,"elapsed":1193,"user":{"displayName":"octaCodeer Sujon","userId":"01146313839386021860"}},"outputId":"2fc969d5-aea7-4423-f5b6-1fa6158d24b9"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["   0     1     2     3     4     5     6     7     8     9     ...  2402  \\\n","0     0     0     0     0     0     0     0     0     0     0  ...     0   \n","1     0     0     0     0     0     0     0     0     0     0  ...     0   \n","2     0     0     0     0     0     0     0     0     0     0  ...     0   \n","3     0     0     0     0     0     0     0     0     0     0  ...     0   \n","4     0     0     0     0     0     0     0     0     0     0  ...     0   \n","\n","   2403  2404  2405  2406  2407  2408  2409  2410  2411  \n","0     0     0     0     0     0     0     0     0     0  \n","1     0     0     0     0     0     0     0     0     0  \n","2     0     0     0     0     0     0     0     0     0  \n","3     0     0     0     0     0     0     0     0     0  \n","4     0     0     0     0     0     0     0     0     0  \n","\n","[5 rows x 2412 columns]\n"]}]}]}